# Анализ причин периодической высокой нагрузки и зависания сервера

## Резюме

Периодические пики нагрузки и зависания вызваны **комбинацией факторов**: отсутствие таймаутов при вызовах внешнего API, одна тяжёлая периодическая задача Celery, дублирование запросов к iiko по организациям, отсутствие лимитов ресурсов в части docker-compose и риск исчерпания соединений к БД.

---

## 1. Критично: отсутствие таймаутов в HTTP-запросах к iiko

**Где:** `backend/apps/iiko_integration/client.py`  
**Что:** Все вызовы `requests.post(...)` выполняются **без параметра `timeout`**.

**Почему это приводит к зависанию:**
- При медленном или «зависшем» ответе iiko запрос ждёт бесконечно.
- Задача Celery (worker) блокируется на этом запросе.
- Если воркеров мало (1–2), один «долгий» запрос занимает воркер на минуты/часы.
- Остальные задачи (в т.ч. отправка заказов) встают в очередь, накапливаются, потребление памяти и нагрузка растут → сервер «зависает».

**Затронутые сценарии:**
- Периодическая задача **sync_all_terminals_stop_lists** (каждые 5 минут) — много вызовов iiko подряд.
- Задача **send_order_to_iiko_task** при создании заказа — один вызов на заказ.

**Рекомендация:** Добавить таймаут (например, 30–60 секунд) ко всем вызовам `requests.post` в `IikoClient` (connect + read).

---

## 2. Периодическая задача Celery: sync_all_terminals_stop_lists

**Где:** `backend/apps/products/tasks.py`, расписание в `config/settings.py` (каждые **300 сек**).

**Что делает задача:**
1. Загружает все активные терминалы с организациями.
2. Для **каждого** терминала:
   - Проверяет, нужно ли синхронизировать (`should_sync_stop_list`) — запрос к БД (aggregate по стоп-листу).
   - Вызывает **get_stop_lists([organization_id])** — один HTTP-запрос к iiko на терминал.
   - Выполняет большую транзакцию: много UPSERT/UPDATE/DELETE по стоп-листу.

**Почему это даёт пики нагрузки:**

- Задача выполняется **одним куском** в одном воркере: все терминалы обрабатываются последовательно.
- Если у одной организации 10 терминалов — делается **10 одинаковых** вызовов `get_stop_lists([organization_id])`. API iiko возвращает стоп-листы по организации целиком, т.е. один запрос на организацию достаточен. Получается лишняя нагрузка на iiko и длительное время работы задачи.
- Каждый вызов iiko без таймаута может «висеть» долго → вся задача и воркер блокируются.
- Много последовательных запросов к БД (агрегаты + массовые обновления) в одном процессе дают всплеск нагрузки на CPU и диск в момент запуска задачи (каждые 5 минут).

**Рекомендации:**
- Делать **один** вызов `get_stop_lists` на организацию и по ответу обновлять стоп-листы для всех терминалов этой организации.
- Либо разбить задачу: одна подзадача на организацию (или на терминал) и вызывать их через `group`/`chain`, чтобы не блокировать один воркер на всю синхронизацию.
- Обязательно добавить таймауты в клиент iiko (п. 1).

---

## 3. Дублирование вызовов iiko по организациям

**Суть:** В цикле по терминалам для каждого терминала вызывается `get_stop_lists([organization_id])`. Терминалы одной организации имеют один и тот же `organization_id`, поэтому для N терминалов одной организации выполняется N одинаковых запросов.

**Эффект:** Рост времени выполнения задачи, лишняя нагрузка на iiko и на сеть. В момент пика (много терминалов) нагрузка на сервер и внешний API максимальна.

**Рекомендация:** Сгруппировать терминалы по `organization_id`, для каждой организации один раз вызвать `get_stop_lists`, затем по ответу обновить стоп-листы для всех терминалов этой организации.

---

## 4. Конфигурация контейнеров (docker-compose)

**Где:** `docker-compose.yml` (основной prod-ориентированный файл).

**Что не так:**
- **backend:** 3 workers × 4 threads (gthread) = до 12 потоков; нет лимитов CPU/RAM → один сервис может занять весь сервер.
- **celery:** Нет `--concurrency`, `--max-tasks-per-child`, лимитов ресурсов. По умолчанию concurrency = число CPU → несколько процессов; без перезапуска воркеров память может расти (утечки/кэши).
- **redis:** Нет `maxmemory` и политики вытеснения → при росте данных Redis может занять всю доступную память и способствовать OOM.
- **celery-beat:** Нет лимитов; при нестабильности может создавать лишнюю нагрузку.

**Эффект:** При пиковой нагрузке (тяжёлая задача + много заказов) контейнеры могут одновременно потреблять больше ресурсов, чем есть на VPS → swap, тормоза, «зависание» сервера.

**Рекомендации:**
- Использовать (или взять за основу) настройки из `docker-compose.coolify.yml` и `VPS_OPTIMIZATION.md`: лимиты CPU/RAM для backend, celery, redis, celery-beat.
- Для Redis задать `maxmemory` и политику (например, `allkeys-lru`).
- Для Celery задать `--concurrency=2`, `--max-tasks-per-child=500` (или меньше), при необходимости уменьшить число воркеров Gunicorn и потоков.

---

## 5. База данных: соединения

**Где:** `config/settings.py` — секция `DATABASES`.

**Что:** Не заданы `CONN_MAX_AGE` и другие опции пула соединений. Каждый запрос/задача открывает новое соединение к PostgreSQL.

**Эффект:** При большом числе воркеров (Gunicorn + Celery) и пиковой нагрузке количество одновременных соединений может приблизиться к `max_connections` в PostgreSQL. Новые запросы начинают ждать соединение или падать → рост нагрузки и ощущение «зависания».

**Рекомендация:** Для production задать ограниченное время жизни соединения, например `'CONN_MAX_AGE': 60`, и при необходимости ограничить число воркеров/потоков под имеющийся `max_connections`.

---

## 6. Поведение при сбоях iiko и повторные попытки

**Где:** Задачи с `max_retries=3` и `default_retry_delay` (10 или 60 сек).

**Что:** При падении или недоступности iiko задачи повторяются. Без таймаутов каждая попытка может «висеть» очень долго. Очередь Celery растёт, воркеры заняты «зависшими» запросами.

**Рекомендация:** Таймауты в клиенте (п. 1) сократят время одной попытки и ускорят срабатывание retry/ошибки, не блокируя воркеры надолго.

---

## 7. Что делать в первую очередь

| Приоритет | Действие |
|-----------|----------|
| 1 | Добавить **таймауты** во все HTTP-запросы в `IikoClient` (30–60 сек). |
| 2 | Оптимизировать **sync_all_terminals_stop_lists**: один вызов `get_stop_lists` на организацию, обновление стоп-листов по всем терминалам этой организации. |
| 3 | Ввести **лимиты ресурсов** для контейнеров (CPU/RAM) и настроить Redis (`maxmemory`, политика), как в `docker-compose.coolify.yml` и `VPS_OPTIMIZATION.md`. |
| 4 | Задать **CONN_MAX_AGE** для БД и при необходимости уменьшить concurrency Celery и число воркеров/потоков Gunicorn. |

После внедрения п. 1 и 2 периодические пики нагрузки и риск «зависания» из-за долгих запросов к iiko и дублирования вызовов должны заметно снизиться.

---

## Внесённые изменения (по итогам анализа)

1. **Таймауты в IikoClient** (`backend/apps/iiko_integration/client.py`):
   - Добавлен `REQUEST_TIMEOUT = (10, 45)` (connect 10 с, read 45 с).
   - Все вызовы `requests.post` выполняются с `timeout=self.REQUEST_TIMEOUT`.

2. **Оптимизация синхронизации стоп-листов**:
   - В `StopListSyncService` добавлены:
     - `apply_stop_list_response(terminal, api_response)` — применение уже полученного ответа API к одному терминалу (только БД).
     - `sync_stop_lists_for_terminals(terminals)` — один запрос к API на организацию, затем обновление всех терминалов этой организации.
   - Задача `sync_all_terminals_stop_lists` переведена на группировку по организации и вызов `sync_stop_lists_for_terminals`: число запросов к iiko = число организаций с терминалами для синхронизации, а не число терминалов.
